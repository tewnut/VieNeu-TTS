import gradio as gr
print("‚è≥ ƒêang kh·ªüi ƒë·ªông... Vui l√≤ng ch·ªù...")
import soundfile as sf
import tempfile
import torch
from vieneu_tts import VieNeuTTS, FastVieNeuTTS
import os
import time
import numpy as np
from typing import Generator, Optional, Tuple
import queue
import threading
import yaml
from utils.core_utils import split_text_into_chunks
from functools import lru_cache
import gc

print("‚è≥ ƒêang kh·ªüi ƒë·ªông VieNeu-TTS...")

# --- CONSTANTS & CONFIG ---
CONFIG_PATH = os.path.join(os.path.dirname(__file__), "config.yaml")
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        _config = yaml.safe_load(f) or {}
except Exception as e:
    raise RuntimeError(f"Kh√¥ng th·ªÉ ƒë·ªçc config.yaml: {e}")

BACKBONE_CONFIGS = _config.get("backbone_configs", {})
CODEC_CONFIGS = _config.get("codec_configs", {})
VOICE_SAMPLES = _config.get("voice_samples", {})

_text_settings = _config.get("text_settings", {})
MAX_CHARS_PER_CHUNK = _text_settings.get("max_chars_per_chunk", 256)
MAX_TOTAL_CHARS_STREAMING = _text_settings.get("max_total_chars_streaming", 3000)

if not BACKBONE_CONFIGS or not CODEC_CONFIGS:
    raise ValueError("config.yaml thi·∫øu backbone_configs ho·∫∑c codec_configs")
if not VOICE_SAMPLES:
    raise ValueError("config.yaml thi·∫øu voice_samples")

# --- 1. MODEL CONFIGURATION ---
# Global model instance
tts = None
current_backbone = None
current_codec = None
model_loaded = False
using_lmdeploy = False

# Cache for reference texts
_ref_text_cache = {}

def should_use_lmdeploy(backbone_choice: str, device_choice: str) -> bool:
    """Determine if we should use LMDeploy backend."""
    if "gguf" in backbone_choice.lower():
        return False
    
    if device_choice == "Auto":
        has_gpu = torch.cuda.is_available()
    elif device_choice == "CUDA":
        has_gpu = torch.cuda.is_available()
    else:
        has_gpu = False
    
    return has_gpu

@lru_cache(maxsize=32)
def get_ref_text_cached(text_path: str) -> str:
    """Cache reference text loading"""
    with open(text_path, "r", encoding="utf-8") as f:
        return f.read()

def cleanup_gpu_memory():
    """Aggressively cleanup GPU memory"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()

def load_model(backbone_choice: str, codec_choice: str, device_choice: str, 
               enable_triton: bool, max_batch_size: int):
    """Load model with optimizations and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    lmdeploy_error_reason = None
    
    yield (
        "‚è≥ ƒêang t·∫£i model v·ªõi t·ªëi ∆∞u h√≥a... L∆∞u √Ω: Qu√° tr√¨nh n√†y s·∫Ω t·ªën th·ªùi gian. Vui l√≤ng ki√™n nh·∫´n.",
        gr.update(interactive=False),
        gr.update(interactive=False)
    )
    
    try:
        # Cleanup before loading new model
        if model_loaded and tts is not None:
            del tts
            cleanup_gpu_memory()
        
        backbone_config = BACKBONE_CONFIGS[backbone_choice]
        codec_config = CODEC_CONFIGS[codec_choice]
        
        use_lmdeploy = should_use_lmdeploy(backbone_choice, device_choice)
        
        if use_lmdeploy:
            lmdeploy_error_reason = None
            print(f"üöÄ Using LMDeploy backend with optimizations")
            
            backbone_device = "cuda"
            
            if "ONNX" in codec_choice:
                codec_device = "cpu"
            else:
                codec_device = "cuda" if torch.cuda.is_available() else "cpu"
            
            print(f"üì¶ Loading optimized model...")
            print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            print(f"   Triton: {'Enabled' if enable_triton else 'Disabled'}")
            print(f"   Max Batch Size: {max_batch_size}")
            
            try:
                tts = FastVieNeuTTS(
                    backbone_repo=backbone_config["repo"],
                    backbone_device=backbone_device,
                    codec_repo=codec_config["repo"],
                    codec_device=codec_device,
                    memory_util=0.3,
                    tp=1,
                    enable_prefix_caching=True,
                    enable_triton=enable_triton,
                    max_batch_size=max_batch_size,
                )
                using_lmdeploy = True
                
                # Pre-cache voice references
                print("üìù Pre-caching voice references...")
                for voice_name, voice_info in VOICE_SAMPLES.items():
                    audio_path = voice_info["audio"]
                    text_path = voice_info["text"]
                    if os.path.exists(audio_path) and os.path.exists(text_path):
                        ref_text = get_ref_text_cached(text_path)
                        tts.get_cached_reference(voice_name, audio_path, ref_text)
                print(f"   ‚úÖ Cached {len(VOICE_SAMPLES)} voices")
                
            except Exception as e:
                import traceback
                traceback.print_exc()
                
                error_str = str(e)
                if "$env:CUDA_PATH" in error_str:
                    lmdeploy_error_reason = "Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CUDA_PATH. Vui l√≤ng c√†i ƒë·∫∑t NVIDIA GPU Computing Toolkit."
                else:
                    lmdeploy_error_reason = f"{error_str}"
                
                yield (
                    f"‚ö†Ô∏è LMDeploy Init Error: {lmdeploy_error_reason}. ƒêang loading model v·ªõi backend m·∫∑c ƒë·ªãnh - t·ªëc ƒë·ªô ch·∫≠m h∆°n so v·ªõi lmdeploy...",
                    gr.update(interactive=False),
                    gr.update(interactive=False)
                )
                time.sleep(1)
                use_lmdeploy = False
                using_lmdeploy = False
        
        if not use_lmdeploy:
            print(f"üì¶ Using original backend")
            
            if device_choice == "Auto":
                if "gguf" in backbone_choice.lower():
                    backbone_device = "gpu" if torch.cuda.is_available() else "cpu"
                else:
                    backbone_device = "cuda" if torch.cuda.is_available() else "cpu"
                
                if "ONNX" in codec_choice:
                    codec_device = "cpu"
                else:
                    codec_device = "cuda" if torch.cuda.is_available() else "cpu"
            else:
                backbone_device = device_choice.lower()
                codec_device = device_choice.lower()
                
                if "ONNX" in codec_choice:
                    codec_device = "cpu"
            
            if "gguf" in backbone_choice.lower() and backbone_device == "cuda":
                backbone_device = "gpu"
            
            print(f"üì¶ Loading model...")
            print(f"   Backbone: {backbone_config['repo']} on {backbone_device}")
            print(f"   Codec: {codec_config['repo']} on {codec_device}")
            
            tts = VieNeuTTS(
                backbone_repo=backbone_config["repo"],
                backbone_device=backbone_device,
                codec_repo=codec_config["repo"],
                codec_device=codec_device
            )
            using_lmdeploy = False
        
        current_backbone = backbone_choice
        current_codec = codec_choice
        model_loaded = True
        
        # Success message with optimization info
        backend_name = "üöÄ LMDeploy (Optimized)" if using_lmdeploy else "üì¶ Standard"
        device_info = "cuda" if use_lmdeploy else (backbone_device if not use_lmdeploy else "N/A")
        
        streaming_support = "‚úÖ C√≥" if backbone_config['supports_streaming'] else "‚ùå Kh√¥ng"
        preencoded_note = "\n‚ö†Ô∏è Codec n√†y c·∫ßn s·ª≠ d·ª•ng pre-encoded codes (.pt files)" if codec_config['use_preencoded'] else ""
        
        opt_info = ""
        if using_lmdeploy and hasattr(tts, 'get_optimization_stats'):
            stats = tts.get_optimization_stats()
            opt_info = (
                f"\n\nüîß T·ªëi ∆∞u h√≥a:"
                f"\n  ‚Ä¢ Triton: {'‚úÖ' if stats['triton_enabled'] else '‚ùå'}"
                f"\n  ‚Ä¢ Max Batch Size: {max_batch_size}"
                f"\n  ‚Ä¢ Reference Cache: {stats['cached_references']} voices"
                f"\n  ‚Ä¢ Prefix Caching: ‚úÖ"
            )
        
        warning_msg = ""
        if lmdeploy_error_reason:
             warning_msg = (
                 f"\n\n‚ö†Ô∏è **C·∫£nh b√°o:** Kh√¥ng th·ªÉ k√≠ch ho·∫°t LMDeploy (Optimized Backend) do l·ªói sau:\n"
                 f"üëâ {lmdeploy_error_reason}\n"
                 f"üí° H·ªá th·ªëng ƒë√£ t·ª± ƒë·ªông chuy·ªÉn v·ªÅ ch·∫ø ƒë·ªô Standard (ch·∫≠m h∆°n)."
             )

        success_msg = (
            f"‚úÖ Model ƒë√£ t·∫£i th√†nh c√¥ng!\n\n"
            f"üîß Backend: {backend_name}\n"
            f"ü¶ú Model Device: {device_info.upper()}\n"
            f"üéµ Codec Device: {codec_device.upper()}{preencoded_note}\n"
            f"üåä Streaming: {streaming_support}{opt_info}{warning_msg}"
        )
        
        yield (
            success_msg,
            gr.update(interactive=True),
            gr.update(interactive=True)
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        model_loaded = False
        using_lmdeploy = False

        if "$env:CUDA_PATH" in str(e):
            yield (
                "‚ùå L·ªói khi t·∫£i model: Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng CUDA_PATH. Vui l√≤ng c√†i ƒë·∫∑t NVIDIA GPU Computing Toolkit (https://developer.nvidia.com/cuda/toolkit)",
                gr.update(interactive=False),
                gr.update(interactive=True)
            )
        else: 
            yield (
                f"‚ùå L·ªói khi t·∫£i model: {str(e)}",
                gr.update(interactive=False),
                gr.update(interactive=True)
            )


# --- 2. DATA & HELPERS ---
GGUF_ALLOWED_VOICES = [
    "Vƒ©nh (nam mi·ªÅn Nam)",
    "B√¨nh (nam mi·ªÅn B·∫Øc)",
    "Ng·ªçc (n·ªØ mi·ªÅn B·∫Øc)",
    "Dung (n·ªØ mi·ªÅn Nam)",
]

def get_voice_options(backbone_choice: str):
    """Filter voice options: GGUF only shows the 4 allowed voices."""
    if "gguf" in backbone_choice.lower():
        return [v for v in GGUF_ALLOWED_VOICES if v in VOICE_SAMPLES]
    return list(VOICE_SAMPLES.keys())

def update_voice_dropdown(backbone_choice: str, current_voice: str):
    options = get_voice_options(backbone_choice)
    new_value = current_voice if current_voice in options else (options[0] if options else None)
    return gr.update(choices=options, value=new_value)

# --- 3. CORE LOGIC FUNCTIONS ---
def load_reference_info(voice_choice: str) -> Tuple[Optional[str], str]:
    """Load reference audio and text with caching"""
    if voice_choice in VOICE_SAMPLES:
        audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        text_path = VOICE_SAMPLES[voice_choice]["text"]
        try:
            if os.path.exists(text_path):
                ref_text = get_ref_text_cached(text_path)
                return audio_path, ref_text
            else:
                return audio_path, "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file text m·∫´u."
        except Exception as e:
            return None, f"‚ùå L·ªói: {str(e)}"
    return None, ""

def synthesize_speech(text: str, voice_choice: str, custom_audio, custom_text: str, 
                     mode_tab: str, generation_mode: str, use_batch: bool):
    """Synthesis with optimization support and max batch size control"""
    global tts, current_backbone, current_codec, model_loaded, using_lmdeploy
    
    if not model_loaded or tts is None:
        yield None, "‚ö†Ô∏è Vui l√≤ng t·∫£i model tr∆∞·ªõc!"
        return
    
    if not text or text.strip() == "":
        yield None, "‚ö†Ô∏è Vui l√≤ng nh·∫≠p vƒÉn b·∫£n!"
        return
    
    raw_text = text.strip()
    
    codec_config = CODEC_CONFIGS[current_codec]
    use_preencoded = codec_config['use_preencoded']
    
    # Setup Reference
    if mode_tab == "custom_mode":
        if custom_audio is None or not custom_text:
            yield None, "‚ö†Ô∏è Thi·∫øu Audio ho·∫∑c Text m·∫´u custom."
            return
        ref_audio_path = custom_audio
        ref_text_raw = custom_text
        ref_codes_path = None
    else:
        if voice_choice not in VOICE_SAMPLES:
            yield None, "‚ö†Ô∏è Vui l√≤ng ch·ªçn gi·ªçng m·∫´u."
            return
        ref_audio_path = VOICE_SAMPLES[voice_choice]["audio"]
        text_path = VOICE_SAMPLES[voice_choice]["text"]
        ref_codes_path = VOICE_SAMPLES[voice_choice]["codes"]
        
        if not os.path.exists(ref_audio_path):
            yield None, "‚ùå Kh√¥ng t√¨m th·∫•y file audio m·∫´u."
            return
        
        ref_text_raw = get_ref_text_cached(text_path)
    
    yield None, "üìÑ ƒêang x·ª≠ l√Ω Reference..."
    
    # Encode or get cached reference
    try:
        if use_preencoded and ref_codes_path and os.path.exists(ref_codes_path):
            ref_codes = torch.load(ref_codes_path, map_location="cpu", weights_only=True)
        else:
            # Use cached reference if available (LMDeploy only)
            if using_lmdeploy and hasattr(tts, 'get_cached_reference') and mode_tab == "preset_mode":
                ref_codes = tts.get_cached_reference(voice_choice, ref_audio_path, ref_text_raw)
            else:
                ref_codes = tts.encode_reference(ref_audio_path)
        
        if isinstance(ref_codes, torch.Tensor):
            ref_codes = ref_codes.cpu().numpy()
    except Exception as e:
        yield None, f"‚ùå L·ªói x·ª≠ l√Ω reference: {e}"
        return
    
    text_chunks = split_text_into_chunks(raw_text, max_chars=MAX_CHARS_PER_CHUNK)
    total_chunks = len(text_chunks)
    
    # === STANDARD MODE ===
    if generation_mode == "Standard (M·ªôt l·∫ßn)":
        backend_name = "LMDeploy" if using_lmdeploy else "Standard"
        batch_info = " (Batch Mode)" if use_batch and using_lmdeploy and total_chunks > 1 else ""
        
        # Show batch size info
        batch_size_info = ""
        if use_batch and using_lmdeploy and hasattr(tts, 'max_batch_size'):
            batch_size_info = f" [Max batch: {tts.max_batch_size}]"
        
        yield None, f"üöÄ B·∫Øt ƒë·∫ßu t·ªïng h·ª£p {backend_name}{batch_info}{batch_size_info} ({total_chunks} ƒëo·∫°n)..."
        
        all_audio_segments = []
        sr = 24000
        silence_pad = np.zeros(int(sr * 0.15), dtype=np.float32)
        
        start_time = time.time()
        
        try:
            # Use batch processing if enabled and using LMDeploy
            if use_batch and using_lmdeploy and hasattr(tts, 'infer_batch') and total_chunks > 1:
                # Show how many mini-batches will be processed
                batch_size = tts.max_batch_size if hasattr(tts, 'max_batch_size') else 8
                num_batches = (total_chunks + batch_size - 1) // batch_size
                
                yield None, f"‚ö° X·ª≠ l√Ω {num_batches} mini-batch(es) (max {batch_size} ƒëo·∫°n/batch)..."
                
                chunk_wavs = tts.infer_batch(text_chunks, ref_codes, ref_text_raw)
                
                for i, chunk_wav in enumerate(chunk_wavs):
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_audio_segments.append(chunk_wav)
                        if i < total_chunks - 1:
                            all_audio_segments.append(silence_pad)
            else:
                # Sequential processing
                for i, chunk in enumerate(text_chunks):
                    yield None, f"‚è≥ ƒêang x·ª≠ l√Ω ƒëo·∫°n {i+1}/{total_chunks}..."
                    
                    chunk_wav = tts.infer(chunk, ref_codes, ref_text_raw)
                    
                    if chunk_wav is not None and len(chunk_wav) > 0:
                        all_audio_segments.append(chunk_wav)
                        if i < total_chunks - 1:
                            all_audio_segments.append(silence_pad)
            
            if not all_audio_segments:
                yield None, "‚ùå Kh√¥ng sinh ƒë∆∞·ª£c audio n√†o."
                return
            
            yield None, "üíæ ƒêang gh√©p file v√† l∆∞u..."
            
            final_wav = np.concatenate(all_audio_segments)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                output_path = tmp.name
            
            process_time = time.time() - start_time
            backend_info = f" (Backend: {'LMDeploy üöÄ' if using_lmdeploy else 'Standard üì¶'})"
            speed_info = f", T·ªëc ƒë·ªô: {len(final_wav)/sr/process_time:.2f}x realtime" if process_time > 0 else ""
            
            yield output_path, f"‚úÖ Ho√†n t·∫•t! (Th·ªùi gian: {process_time:.2f}s{speed_info}){backend_info}"
            
            # Cleanup memory
            if using_lmdeploy and hasattr(tts, 'cleanup_memory'):
                tts.cleanup_memory()
            cleanup_gpu_memory()
            
        except torch.cuda.OutOfMemoryError as e:
            cleanup_gpu_memory()
            yield None, (
                f"‚ùå GPU h·∫øt VRAM! H√£y th·ª≠:\n"
                f"‚Ä¢ Gi·∫£m Max Batch Size (hi·ªán t·∫°i: {tts.max_batch_size if hasattr(tts, 'max_batch_size') else 'N/A'})\n"
                f"‚Ä¢ Gi·∫£m ƒë·ªô d√†i vƒÉn b·∫£n\n\n"
                f"Chi ti·∫øt: {str(e)}"
            )
            return
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            cleanup_gpu_memory()
            yield None, f"‚ùå L·ªói Standard Mode: {str(e)}"
            return
    
    # === STREAMING MODE ===
    else:
        sr = 24000
        crossfade_samples = int(sr * 0.03)
        audio_queue = queue.Queue(maxsize=100)
        PRE_BUFFER_SIZE = 3
        
        end_event = threading.Event()
        error_event = threading.Event()
        error_msg = ""
        
        def producer_thread():
            nonlocal error_msg
            try:
                previous_tail = None
                
                for i, chunk_text in enumerate(text_chunks):
                    stream_gen = tts.infer_stream(chunk_text, ref_codes, ref_text_raw)
                    
                    for part_idx, audio_part in enumerate(stream_gen):
                        if audio_part is None or len(audio_part) == 0:
                            continue
                        
                        if previous_tail is not None and len(previous_tail) > 0:
                            overlap = min(len(previous_tail), len(audio_part), crossfade_samples)
                            if overlap > 0:
                                fade_out = np.linspace(1.0, 0.0, overlap, dtype=np.float32)
                                fade_in = np.linspace(0.0, 1.0, overlap, dtype=np.float32)
                                
                                blended = (audio_part[:overlap] * fade_in + 
                                         previous_tail[-overlap:] * fade_out)
                                
                                processed = np.concatenate([
                                    previous_tail[:-overlap] if len(previous_tail) > overlap else np.array([]),
                                    blended,
                                    audio_part[overlap:]
                                ])
                            else:
                                processed = np.concatenate([previous_tail, audio_part])
                            
                            tail_size = min(crossfade_samples, len(processed))
                            previous_tail = processed[-tail_size:].copy()
                            output_chunk = processed[:-tail_size] if len(processed) > tail_size else processed
                        else:
                            tail_size = min(crossfade_samples, len(audio_part))
                            previous_tail = audio_part[-tail_size:].copy()
                            output_chunk = audio_part[:-tail_size] if len(audio_part) > tail_size else audio_part
                        
                        if len(output_chunk) > 0:
                            audio_queue.put((sr, output_chunk))
                
                if previous_tail is not None and len(previous_tail) > 0:
                    audio_queue.put((sr, previous_tail))
                    
            except Exception as e:
                import traceback
                traceback.print_exc()
                error_msg = str(e)
                error_event.set()
            finally:
                end_event.set()
                audio_queue.put(None)
        
        threading.Thread(target=producer_thread, daemon=True).start()
        
        yield (sr, np.zeros(int(sr * 0.05))), "üìÑ ƒêang buffering..."
        
        pre_buffer = []
        while len(pre_buffer) < PRE_BUFFER_SIZE:
            try:
                item = audio_queue.get(timeout=5.0)
                if item is None:
                    break
                pre_buffer.append(item)
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    return
                break
        
        full_audio_buffer = []
        backend_info = "üöÄ LMDeploy" if using_lmdeploy else "üì¶ Standard"
        for sr, audio_data in pre_buffer:
            full_audio_buffer.append(audio_data)
            yield (sr, audio_data), f"üîä ƒêang ph√°t ({backend_info})..."
        
        while True:
            try:
                item = audio_queue.get(timeout=0.05)
                if item is None:
                    break
                sr, audio_data = item
                full_audio_buffer.append(audio_data)
                yield (sr, audio_data), f"üîä ƒêang ph√°t ({backend_info})..."
            except queue.Empty:
                if error_event.is_set():
                    yield None, f"‚ùå L·ªói: {error_msg}"
                    break
                if end_event.is_set() and audio_queue.empty():
                    break
                continue
        
        if full_audio_buffer:
            final_wav = np.concatenate(full_audio_buffer)
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, final_wav, sr)
                yield tmp.name, f"‚úÖ Ho√†n t·∫•t Streaming! ({backend_info})"
            
            # Cleanup memory
            if using_lmdeploy and hasattr(tts, 'cleanup_memory'):
                tts.cleanup_memory()
            cleanup_gpu_memory()


# --- 4. UI SETUP ---
theme = gr.themes.Soft(
    primary_hue="indigo",
    secondary_hue="cyan",
    neutral_hue="slate",
    font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui'],
).set(
    button_primary_background_fill="linear-gradient(90deg, #6366f1 0%, #0ea5e9 100%)",
    button_primary_background_fill_hover="linear-gradient(90deg, #4f46e5 0%, #0284c7 100%)",
)

css = """
.container { max-width: 1400px; margin: auto; }
.header-box {
    text-align: center;
    margin-bottom: 25px;
    padding: 25px;
    background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
    border-radius: 12px;
    color: white !important;
}
.header-title {
    font-size: 2.5rem;
    font-weight: 800;
    color: white !important;
}
.gradient-text {
    background: -webkit-linear-gradient(45deg, #60A5FA, #22D3EE);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
}
.header-icon {
    color: white;
}
.status-box {
    font-weight: bold;
    text-align: center;
    border: none;
    background: transparent;
}
.model-card-content {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    align-items: center;
    gap: 15px;
    font-size: 0.9rem;
    text-align: center;
    color: white !important;
}
.model-card-item {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 6px;
    color: white !important;
}
.model-card-item strong {
    color: white !important;
}
.model-card-item span {
    color: white !important;
}
.model-card-link {
    color: #60A5FA;
    text-decoration: none;
    font-weight: 500;
    transition: color 0.2s;
}
.model-card-link:hover {
    color: #22D3EE;
    text-decoration: underline;
}
"""

EXAMPLES_LIST = [
    ["V·ªÅ mi·ªÅn T√¢y kh√¥ng ch·ªâ ƒë·ªÉ ng·∫Øm nh√¨n s√¥ng n∆∞·ªõc h·ªØu t√¨nh, m√† c√≤n ƒë·ªÉ c·∫£m nh·∫≠n t·∫•m ch√¢n t√¨nh c·ªßa ng∆∞·ªùi d√¢n n∆°i ƒë√¢y.", "Vƒ©nh (nam mi·ªÅn Nam)"],
    ["H√† N·ªôi nh·ªØng ng√†y v√†o thu mang m·ªôt v·∫ª ƒë·∫πp tr·∫ßm m·∫∑c v√† c·ªï k√≠nh ƒë·∫øn l·∫° th∆∞·ªùng.", "B√¨nh (nam mi·ªÅn B·∫Øc)"],
]

with gr.Blocks(theme=theme, css=css, title="VieNeu-TTS") as demo:
    with gr.Column(elem_classes="container"):
        gr.HTML("""
<div class="header-box">
    <h1 class="header-title">
        <span class="header-icon">ü¶ú</span>
        <span class="gradient-text">VieNeu-TTS Studio</span>
    </h1>
    <div class="model-card-content">
        <div class="model-card-item">
            <strong>Models:</strong>
            <a href="https://huggingface.co/pnnbao-ump/VieNeu-TTS" target="_blank" class="model-card-link">VieNeu-TTS</a>
            <span>‚Ä¢</span>
            <a href="https://huggingface.co/pnnbao-ump/VieNeu-TTS-q4-gguf" target="_blank" class="model-card-link">Q4-GGUF</a>
            <span>‚Ä¢</span>
            <a href="https://huggingface.co/pnnbao-ump/VieNeu-TTS-q8-gguf" target="_blank" class="model-card-link">Q8-GGUF</a>
        </div>
        <div class="model-card-item">
            <strong>Repository:</strong>
            <a href="https://github.com/pnnbao97/VieNeu-TTS" target="_blank" class="model-card-link">GitHub</a>
        </div>
        <div class="model-card-item">
            <strong>T√°c gi·∫£:</strong>
            <span>Ph·∫°m Nguy·ªÖn Ng·ªçc B·∫£o</span>
        </div>
    </div>
</div>
        """)
        
        # --- CONFIGURATION ---
        with gr.Group():
            with gr.Row():
                backbone_select = gr.Dropdown(list(BACKBONE_CONFIGS.keys()), value="VieNeu-TTS (GPU)", label="ü¶ú Backbone")
                codec_select = gr.Dropdown(list(CODEC_CONFIGS.keys()), value="NeuCodec (Standard)", label="üéµ Codec")
                device_choice = gr.Radio(["Auto", "CPU", "CUDA"], value="Auto", label="üñ•Ô∏è Device")
            
            with gr.Row():
                enable_triton = gr.Checkbox(value=True, label="‚ö° Enable Triton Compilation")
                max_batch_size = gr.Slider(
                    minimum=1, 
                    maximum=16, 
                    value=8, 
                    step=1, 
                    label="üìä Max Batch Size",
                    info="Gi·∫£m n·∫øu g·∫∑p l·ªói OOM. 4-6 cho GPU 8GB, 8-12 cho GPU 16GB+"
                )
            
            gr.Markdown(
                "‚ö†Ô∏è **L∆∞u √Ω:** N·∫øu m√°y b·∫°n ch·ªâ c√≥ CPU vui l√≤ng ch·ªçn phi√™n b·∫£n GGUF (Q4/Q8) ƒë·ªÉ c√≥ t·ªëc ƒë·ªô nhanh nh·∫•t.\n\n"
                "üí° **Max Batch Size:** S·ªë l∆∞·ª£ng ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c x·ª≠ l√Ω c√πng l√∫c. "
                "Gi√° tr·ªã cao = nhanh h∆°n nh∆∞ng t·ªën VRAM h∆°n. Gi·∫£m xu·ªëng n·∫øu g·∫∑p l·ªói \"Out of Memory\"."
            )

            btn_load = gr.Button("üîÑ T·∫£i Model", variant="primary")
            model_status = gr.Markdown("‚è≥ Ch∆∞a t·∫£i model.")
        
        with gr.Row(elem_classes="container"):
            # --- INPUT ---
            with gr.Column(scale=3):
                text_input = gr.Textbox(
                    label=f"VƒÉn b·∫£n (Streaming h·ªó tr·ª£ t·ªõi {MAX_TOTAL_CHARS_STREAMING} k√Ω t·ª±, chia chunk {MAX_CHARS_PER_CHUNK} k√Ω t·ª±)",
                    lines=4,
                    value="H√† N·ªôi, tr√°i tim c·ªßa Vi·ªát Nam, l√† m·ªôt th√†nh ph·ªë ng√†n nƒÉm vƒÉn hi·∫øn v·ªõi b·ªÅ d√†y l·ªãch s·ª≠ v√† vƒÉn h√≥a ƒë·ªôc ƒë√°o. B∆∞·ªõc ch√¢n tr√™n nh·ªØng con ph·ªë c·ªï k√≠nh quanh H·ªì Ho√†n Ki·∫øm, du kh√°ch nh∆∞ ƒë∆∞·ª£c du h√†nh ng∆∞·ª£c th·ªùi gian, chi√™m ng∆∞·ª°ng ki·∫øn tr√∫c Ph√°p c·ªï ƒëi·ªÉn h√≤a quy·ªán v·ªõi n√©t ki·∫øn tr√∫c truy·ªÅn th·ªëng Vi·ªát Nam. M·ªói con ph·ªë trong khu ph·ªë c·ªï mang m·ªôt t√™n g·ªçi ƒë·∫∑c tr∆∞ng, ph·∫£n √°nh ngh·ªÅ th·ªß c√¥ng truy·ªÅn th·ªëng t·ª´ng th·ªãnh h√†nh n∆°i ƒë√¢y nh∆∞ ph·ªë H√†ng B·∫°c, H√†ng ƒê√†o, H√†ng M√£. ·∫®m th·ª±c H√† N·ªôi c≈©ng l√† m·ªôt ƒëi·ªÉm nh·∫•n ƒë·∫∑c bi·ªát, t·ª´ t√¥ ph·ªü n√≥ng h·ªïi bu·ªïi s√°ng, b√∫n ch·∫£ th∆°m l·ª´ng tr∆∞a h√®, ƒë·∫øn ch√® Th√°i ng·ªçt ng√†o chi·ªÅu thu. Nh·ªØng m√≥n ƒÉn d√¢n d√£ n√†y ƒë√£ tr·ªü th√†nh bi·ªÉu t∆∞·ª£ng c·ªßa vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát, ƒë∆∞·ª£c c·∫£ th·∫ø gi·ªõi y√™u m·∫øn. Ng∆∞·ªùi H√† N·ªôi n·ªïi ti·∫øng v·ªõi t√≠nh c√°ch hi·ªÅn h√≤a, l·ªãch thi·ªáp nh∆∞ng c≈©ng r·∫•t c·∫ßu to√†n trong t·ª´ng chi ti·∫øt nh·ªè, t·ª´ c√°ch pha tr√† sen cho ƒë·∫øn c√°ch ch·ªçn hoa sen t√¢y ƒë·ªÉ th∆∞·ªüng tr√†.",
                )
                
                with gr.Tabs() as tabs:
                    with gr.TabItem("üë§ Preset", id="preset_mode") as tab_preset:
                        initial_voices = get_voice_options("VieNeu-TTS (GPU)")
                        default_voice = initial_voices[0] if initial_voices else None
                        voice_select = gr.Dropdown(initial_voices, value=default_voice, label="Gi·ªçng m·∫´u")
                    
                    with gr.TabItem("ü¶ú Voice Cloning", id="custom_mode") as tab_custom:
                        custom_audio = gr.Audio(label="Audio gi·ªçng m·∫´u (10-15 gi√¢y) (.wav)", type="filepath")
                        custom_text = gr.Textbox(label="N·ªôi dung audio m·∫´u - vui l√≤ng g√µ ƒë√∫ng n·ªôi dung c·ªßa audio m·∫´u - k·ªÉ c·∫£ d·∫•u c√¢u v√¨ model r·∫•t nh·∫°y c·∫£m v·ªõi d·∫•u c√¢u (.,?!)")
                
                generation_mode = gr.Radio(
                    ["Standard (M·ªôt l·∫ßn)"],
                    value="Standard (M·ªôt l·∫ßn)",
                    label="Ch·∫ø ƒë·ªô sinh"
                )
                use_batch = gr.Checkbox(
                    value=True, 
                    label="‚ö° Batch Processing",
                    info="X·ª≠ l√Ω nhi·ªÅu ƒëo·∫°n c√πng l√∫c (ch·ªâ √°p d·ª•ng khi s·ª≠ d·ª•ng GPU v√† ƒë√£ c√†i ƒë·∫∑t LMDeploy)"
                )
                
                # State to track current mode (replaces unreliable Textbox/Tabs input)
                current_mode_state = gr.State("preset_mode")
                
                btn_generate = gr.Button("üéµ B·∫Øt ƒë·∫ßu", variant="primary", size="lg", interactive=False)
            
            # --- OUTPUT ---
            with gr.Column(scale=2):
                audio_output = gr.Audio(
                    label="K·∫øt qu·∫£",
                    type="filepath",
                    autoplay=True
                )
                status_output = gr.Textbox(label="Tr·∫°ng th√°i", elem_classes="status-box")
        
        # --- EVENT HANDLERS ---
        def update_info(backbone: str) -> str:
            return f"Streaming: {'‚úÖ' if BACKBONE_CONFIGS[backbone]['supports_streaming'] else '‚ùå'}"
        
        backbone_select.change(update_info, backbone_select, model_status)
        backbone_select.change(update_voice_dropdown, [backbone_select, voice_select], voice_select)
        
        # Bind tab events to update state
        tab_preset.select(lambda: "preset_mode", outputs=current_mode_state)
        tab_custom.select(lambda: "custom_mode", outputs=current_mode_state)
        
        btn_load.click(
            fn=load_model,
            inputs=[backbone_select, codec_select, device_choice, enable_triton, max_batch_size],
            outputs=[model_status, btn_generate, btn_load]
        )
        
        btn_generate.click(
            fn=synthesize_speech,
            inputs=[text_input, voice_select, custom_audio, custom_text, current_mode_state, generation_mode, use_batch],
            outputs=[audio_output, status_output]
        )

if __name__ == "__main__":
    # Cho ph√©p override t·ª´ bi·∫øn m√¥i tr∆∞·ªùng (h·ªØu √≠ch cho Docker)
    server_name = os.getenv("GRADIO_SERVER_NAME", "127.0.0.1")
    server_port = int(os.getenv("PORT", os.getenv("GRADIO_SERVER_PORT", "7860")))
    demo.queue().launch(server_name=server_name, server_port=server_port)
